{"componentChunkName":"component---src-templates-blog-post-js","path":"/big-o-omega-theta/","result":{"data":{"site":{"siteMetadata":{"title":"jarednielsen.com"}},"markdownRemark":{"id":"e165849c-9025-5f64-9501-55ee16a29b2a","excerpt":"Is there a computer science topic more terrifying than Big O notation? Don’t let the name scare you, Big O notation is not a big deal. It’s very easy to…","html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1c032ab0f11464bfe0a6ff3594f6b5ff/29007/jarednielsen-big-o-omega-theta.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABbElEQVR42pVRy0oDQRDcr5ZcBUUkENT4gIgGgyISPQTBSBREkYSAGg/iQfAVEVHRPHZnp2emt1t6siQKubjUoWe7aqa6OmC0AudhDQOwAXZGYIBBy09nfEuPWp4fEFpK9YYTZPlIqNYwJXJKUDSDFtFQKeL0ZQvCenpRuTWoHAub0NTOVX4D75/lhvarmivqvUMm9F6GYnGomTmaLcDRmd7ed/VL6vai7Kqu1NTSJjPHuXVbv+hPzOBDO7Um4oEHA8ysi7u22YJyFVs3yfunmi+ZxpVaKDGzmszj20c4tYi3d+LfAuNQLLbZHJz2M9leJouPbSaEcjUu7LjmNTO7k0Y0vRyvbP3OLKDEUeIGk1OsCDTpmOJYjlrT1zejk5zRUacju3A2zQzTmf3LBkhFFIbU71Pki54vul0Kw6TbIxVTGLJSEpARfkDoBtsSGCDQfrcw2rMBiccAa52u3Q5tkxsPP86oGIfgD/Wf+AFpK3A+aqbwXwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"jarednielsen big o omega theta\"\n        title=\"\"\n        src=\"/static/1c032ab0f11464bfe0a6ff3594f6b5ff/f058b/jarednielsen-big-o-omega-theta.png\"\n        srcset=\"/static/1c032ab0f11464bfe0a6ff3594f6b5ff/c26ae/jarednielsen-big-o-omega-theta.png 158w,\n/static/1c032ab0f11464bfe0a6ff3594f6b5ff/6bdcf/jarednielsen-big-o-omega-theta.png 315w,\n/static/1c032ab0f11464bfe0a6ff3594f6b5ff/f058b/jarednielsen-big-o-omega-theta.png 630w,\n/static/1c032ab0f11464bfe0a6ff3594f6b5ff/40601/jarednielsen-big-o-omega-theta.png 945w,\n/static/1c032ab0f11464bfe0a6ff3594f6b5ff/78612/jarednielsen-big-o-omega-theta.png 1260w,\n/static/1c032ab0f11464bfe0a6ff3594f6b5ff/29007/jarednielsen-big-o-omega-theta.png 1600w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Is there a computer science topic more terrifying than Big O notation? Don’t let the name scare you, Big O notation is not a big deal. It’s very easy to understand and you don’t need to be a math whiz to do so. Big O, Big Omega, or Ω, and Big Theta, or Θ, are notations used to express the computational complexity of an algorithm. In this tutorial, you’ll learn the difference between Big O, Big Omega, and Big Theta notations.</p>\n<p>If you’re just joining us, you may want to start at the beginning with <a href=\"https://jarednielsen.com/big-o-notation/\">What is Big O Notation?</a>.</p>\n<hr>\n<p><img src=\"./../../assets/graphics/little-book-big-o/jarednielsen-little-book-big-o-banner.png\" alt=\"\"></p>\n<p>Be O(#1). Grab your copy of <a href=\"https://gum.co/big-o\">The Little Book of Big O</a>.</p>\n<hr>\n<h2>What is Asymptotic Computational Complexity?</h2>\n<p>Here’s the <a href=\"https://en.wikipedia.org/wiki/Asymptotic_computational_complexity\">Wikipedia definition of asymptotic computational complexity</a>:</p>\n<blockquote>\n<p>In computational complexity theory, asymptotic computational complexity is the usage of asymptotic analysis for the estimation of computational complexity of algorithms and computational problems, commonly associated with the usage of the big O notation.</p>\n</blockquote>\n<p>😪</p>\n<p>Right?</p>\n<p>No wonder this stuff is hard to learn.</p>\n<p>Let’s break this jargon down.</p>\n<h3>What is an Asymptote?</h3>\n<p>Here’s Wikipedia again, telling us that an asymptote of a curve is:</p>\n<blockquote>\n<p>a line such that the distance between the curve and the line approaches zero as one or both of the x or y coordinates tends to infinity.</p>\n</blockquote>\n<p>😕</p>\n<p>Yeah.</p>\n<p>These definitions make an easy concept harder to understand.</p>\n<p>We’re all visual learners, so let’s chart the following equation:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">y = 1 / x</code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/32c1589243f13448fac04d22cb735af9/c1b63/desmos-asymptote-01.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.45569620253164%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABSklEQVR42qVT7W6DMAzk/R8StYICGVBKEkoCgZvOJajr1q7afljByfnjziaZ5xnTNIGncw7R997LGZYFbf0B3dTyzbufcPRpyauEtGVZYLWGrt9MeH85juMOYhAthIBL2+KiFOYQ5O0RF+Npyf3l9XrdQQyiMcm5adBVFaaNxSMuxtPepmzqGgsgBZz3CM8okxIrCx0+bH60sK4YrIVWCn4Y4NkdCz/g6FPjpM4y9EoJpSJN0ZUl2qJAdTziI8tEu+J0gipLdEqhKUtUhwPOG+6UpjdcVcl3Yo2Rim4c0ff9TsNaexPfObR5Dq31PhS+RZzRWjSlb4xBIqvwjPI0iW6mbTBoLROnTkw0b3HfKMf1eDXlOs/Rd92+g/+b8rqKvjZ2+NfFjh2QBn872/e3JL8tdtQlbIJHX6pSw3UFN4Edciejhl9wWzztE7iK9D05RtCKAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"desmos asymptote 01\"\n        title=\"\"\n        src=\"/static/32c1589243f13448fac04d22cb735af9/f058b/desmos-asymptote-01.png\"\n        srcset=\"/static/32c1589243f13448fac04d22cb735af9/c26ae/desmos-asymptote-01.png 158w,\n/static/32c1589243f13448fac04d22cb735af9/6bdcf/desmos-asymptote-01.png 315w,\n/static/32c1589243f13448fac04d22cb735af9/f058b/desmos-asymptote-01.png 630w,\n/static/32c1589243f13448fac04d22cb735af9/40601/desmos-asymptote-01.png 945w,\n/static/32c1589243f13448fac04d22cb735af9/c1b63/desmos-asymptote-01.png 1200w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>No matter how large (or small) the value of <em>x</em>, our curve will never touch the <em>x</em> or <em>y</em> axes.</p>\n<p>Even if that number is <em>Infinity</em>.</p>\n<p>🐢🏃‍♀️</p>\n<p>Especially if that number is <em>zero</em>.</p>\n<p>Why?</p>\n<p>It’s mathematically impossible to divide by 0.</p>\n<p>Division is another way to think about this. How many times can we divide one in half in before we reach zero?</p>\n<p>There are infinite divisions!</p>\n<p>Half of 1 is <code class=\"language-text\">½</code>.</p>\n<p>Half of <code class=\"language-text\">½</code> is <code class=\"language-text\">¼</code>.</p>\n<p>Half of <code class=\"language-text\">¼</code> is <code class=\"language-text\">⅛</code>.</p>\n<p>Half of <code class=\"language-text\">⅛</code> is <code class=\"language-text\">1/16</code>.</p>\n<p>Half of <code class=\"language-text\">1/16</code> is… you see where this is going.</p>\n<p>With each division, our denominator increases by a power of 2.</p>\n<p>We will never be able to halve something into nothing.</p>\n<p>Unless it’s cake.</p>\n<p>🍰</p>\n<p>In the chart above, the <em>x</em> and <em>y</em> axes are the asymptotes of the equation <code class=\"language-text\">y = 1 / x</code>. But any line can be an asymptote. We’re not limited to horizontal and vertical lines.</p>\n<h3>What is Asymptotic Analysis?</h3>\n<p>Let’s continue to digest these chewy definitions.</p>\n<p>According to <a href=\"https://en.wikipedia.org/wiki/Asymptotic_analysis\">Wikipedia</a>, asymptotic analysis is:</p>\n<blockquote>\n<p>a method of describing limiting behavior.</p>\n</blockquote>\n<p>What’s a limit?</p>\n<p>In mathematics, a limit is the value that a function approaches as the input increases (or decreases).</p>\n<p>Sounds a lot like an asymptote. So how do we analyze a function <em>asymptotically</em>?</p>\n<p>If our function, <em>f(x)</em> is equal to <code class=\"language-text\">x^2 + 2x</code>, as <code class=\"language-text\">x</code> increases in value (and approaches infinity), <code class=\"language-text\">2x</code> becomes insignificant compared to <code class=\"language-text\">x^2</code>.</p>\n<p>We then simply say that <em>f(x)</em> is asymptotically equivalent to x^2.</p>\n<h2>What’s the Difference Between Big O, Big Omega, and Big Theta?</h2>\n<p>In <a href=\"https://amzn.to/2KdvlNi\">Introduction to Algorithms</a>, Cormen et al use Insertion Sort as an example to explain the difference between Big O, Big Omega, and Big Theta. We’ll do the same here. Here’s an implementation of Insertion Sort in JavaScript:</p>\n<div class=\"gatsby-highlight\" data-language=\"js\"><pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">const</span> <span class=\"token function-variable function\">insertionSort</span> <span class=\"token operator\">=</span> <span class=\"token parameter\">arr</span> <span class=\"token operator\">=></span> <span class=\"token punctuation\">{</span>\n <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">let</span> i <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> arr<span class=\"token punctuation\">.</span>length<span class=\"token punctuation\">;</span> i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n   <span class=\"token keyword\">let</span> index <span class=\"token operator\">=</span> i<span class=\"token punctuation\">;</span>\n   <span class=\"token keyword\">while</span> <span class=\"token punctuation\">(</span>index <span class=\"token operator\">></span> <span class=\"token number\">0</span> <span class=\"token operator\">&amp;&amp;</span> arr<span class=\"token punctuation\">[</span>index <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> arr<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n     <span class=\"token keyword\">let</span> temp <span class=\"token operator\">=</span> arr<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n     arr<span class=\"token punctuation\">[</span>index<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> arr<span class=\"token punctuation\">[</span>index <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n     arr<span class=\"token punctuation\">[</span>index <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> temp<span class=\"token punctuation\">;</span>\n     index <span class=\"token operator\">=</span> index <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span>\n   <span class=\"token punctuation\">}</span>\n <span class=\"token punctuation\">}</span>\n <span class=\"token keyword\">return</span> arr<span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span>\n</code></pre></div>\n<p>And here’s a quick refresher (or explainer) on Insertion Sort:</p>\n<ul>\n<li>\n<p>We iterate over the entire array.</p>\n</li>\n<li>\n<p>For each element in the array, we check:</p>\n<ul>\n<li>\n<p>If the value of the index is greater than 1</p>\n</li>\n<li>\n<p>If the element preceding the index is greater than the index itself</p>\n</li>\n</ul>\n</li>\n<li>\n<p>If both conditions are met, we swap them</p>\n</li>\n</ul>\n<h3>What is Big O?</h3>\n<p>Big O describes the upper bound of an algorithm.</p>\n<p>Using our Insertion Sort example, the rate of growth of our algorithm is <em>at most</em> quadratic, or O(n^2).</p>\n<p>This is why, for us, as developers and practitioners, we are primarily concerned with Big O.</p>\n<p>We want to know just how poorly an algorithm might perform.</p>\n<p>To say that the Big O of Insertion Sort is O(n^2) doesn’t mean that Insertion Sort will <em>always</em> be O(n^2). The actual runtime will vary based on the input. What we are saying is that, in a worst case scenario, this is the upper bound of the performance of Insertion Sort.</p>\n<p>Knowing this, we then need to ask ourselves the following questions:</p>\n<ul>\n<li>\n<p>Are we okay with this?</p>\n</li>\n<li>\n<p>Can we do better?</p>\n</li>\n</ul>\n<h3>What is Big Omega?</h3>\n<p>Big Omega describes the lower bound of an algorithm.</p>\n<p>Using our Insertion Sort example, if the input is already sorted, then the rate of growth of our algorithm is <em>at least</em> linear, or Ω(n).</p>\n<p>If only life always handed us sorted arrays. 🌼</p>\n<p>We can also think of this as our best-case scenario.</p>\n<h3>What is Big Theta?</h3>\n<p>Big Theta describes the tight bound of an algorithm, it’s limit from above <em>and</em> below.</p>\n<p>Using our Insertion Sort example, we know that the rate of growth is <em>at most</em> O(n^2) and <em>at least</em> Ω(n).</p>\n<p>But Big Theta will change with our inputs. Returning to our Insertion Sort example, in a best case scenario, Big Theta is <em>n</em>, but in a worst case scenario, Big Theta is <em>n^2</em>. To say that Insertion Sort <em>is</em> Θ(n) or Θ(n^2) is incorrect as both of these imply that Insertion Sort will <em>always</em> run at either <em>n</em> or <em>n^2</em>.</p>\n<p>So what use is Big Theta?</p>\n<p>Big Theta is often used to describe the average, or expected, case for an algorithm. This isn’t <em>exactly</em> true, but it’s a useful shorthand.</p>\n<p>So for Insertion Sort, the average case for Big Theta is <em>n^2</em>.</p>\n<h3>So What’s the Difference Between Big O, Big Omega, and Big Theta?</h3>\n<p>We can think of Big O, Big Omega, and Big Theta like conditional operators:</p>\n<ul>\n<li>\n<p>Big O is like <code class=\"language-text\">&lt;=</code>, meaning the rate of growth of an algorithm is less than or equal to a specific value, e.g: <em>f(x)</em> &#x3C;= <em>O(n^2)</em></p>\n</li>\n<li>\n<p>Big Omega is like <code class=\"language-text\">>=</code>, meaning the rate of growth is greater than or equal to a specified value, e.g: <em>f(x)</em> >= <em>Ω(n)</em>.</p>\n</li>\n<li>\n<p>Big Theta is like <code class=\"language-text\">==</code>, meaning the rate of growth is equal to a specified value, e.g: <em>f(x)</em> = <em>Θ(n^2)</em>.</p>\n</li>\n</ul>\n<p>But if we needed to make a blanket statement for all cases, we would use Big O, and, for example, say that Insertion Sort is O(n^2).</p>\n<h2>Best Case, Worst Case, and Expected Case</h2>\n<p>What is the relationship between best case/worst case/expected case and Big O/Big Omega/Big Theta?</p>\n<p>There isn’t one.</p>\n<p>Equivalencies are often made between Big O and worst case, Big Omega and best case, and Big Theta and average case, but we can speak of best, worst, and average for each of these notations.</p>\n<p>For example, each of the following statements are true:</p>\n<ul>\n<li>\n<p>Insertion Sort’s worst case rate of growth is <em>at most</em> O(n^2)</p>\n</li>\n<li>\n<p>Insertion Sort’s worst case rate of growth is <em>at least</em> Ω(n)</p>\n</li>\n<li>\n<p>Insertion Sort’s worst case rate of growth is <em>exactly</em> Θ(n^2)</p>\n</li>\n</ul>\n<h2>Big O, Big Omega, Big Theta, Big Sigh!</h2>\n<p>In this tutorial, you learned the difference between Big O, Big Omega, and Big Theta notations.</p>","frontmatter":{"title":"What’s the Difference Between Big O, Big Omega, and Big Theta?","date":"April 10, 2020","description":"Big O is only one of many notations for analyzing the complexity of algorithms. In this tutorial, you’ll learn the difference between Big O, Big Omega, and Big Theta notations."}},"previous":{"fields":{"slug":"/learning-memory-retrieval/"},"frontmatter":{"title":"How to Improve Learning and Memory with Retrieval Practice"}},"next":{"fields":{"slug":"/analogy-learning-software-development/"},"frontmatter":{"title":"Analogy in Learning and Software Development"}}},"pageContext":{"id":"e165849c-9025-5f64-9501-55ee16a29b2a","previousPostId":"c38d1a88-cb94-5065-92a7-17db594c0e08","nextPostId":"db09e797-c6e8-5277-9a27-f74a9bf3f475"}},"staticQueryHashes":["2841359383","3257411868"],"slicesMap":{}}